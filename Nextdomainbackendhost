from flask import Flask, request, jsonify
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

app = Flask(__name__)

def is_valid_url(url):
    parsed = urlparse(url)
    return parsed.scheme and parsed.netloc

@app.route("/scan")
def scan():
    url = request.args.get("url")

    if not is_valid_url(url):
        return jsonify({"error": "Invalid URL"})

    try:
        r = requests.get(url, timeout=5)
        soup = BeautifulSoup(r.text, "html.parser")

        pages = []
        hidden = []

        for a in soup.find_all("a", href=True):
            full = urljoin(url, a["href"])
            if url in full:
                pages.append(full)
            elif a["href"].startswith("/"):
                hidden.append(urljoin(url, a["href"]))

        return jsonify({
            "title": soup.title.string if soup.title else "No title",
            "pages": pages[:20],
            "hidden": hidden[:20],
            "headers": dict(r.headers)
        })

    except Exception as e:
        return jsonify({"error": str(e)})

app.run(host="0.0.0.0", port=10000)
